[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AgroClimate Cookbook",
    "section": "",
    "text": "Preface\nThank you for checking out our book.\nWe would like to thank…"
  },
  {
    "objectID": "01_data-wrangling.html#load-packages",
    "href": "01_data-wrangling.html#load-packages",
    "title": "1  Data Wrangling Methods",
    "section": "1.1 Load packages",
    "text": "1.1 Load packages\nThe tidyverse provides some generically useful packages for data wrangling.\nAs usual, start by loading a bunch of packages into memory and specifying our package preferences for conflicting function names:\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(lubridate)\n\n# MOVE THESE BELOW\nlibrary(caladaptr)\nlibrary(units)\nlibrary(sf)\n\nWe’ll be using functions from dplyr quite a bit. Unfortunately some of the functions from dplyr clash with functions with the same name from other packages. For example several packages have a function called select().\nAn easy way to avoid function name clashes is to use the conflicted package to specify your preferences when commonly named functions are called. Typically you do this at the top of your script, so you don’t have to worry about it after that.\n\nlibrary(conflicted)\nconflict_prefer(\"filter\", \"dplyr\", quiet = TRUE)\nconflict_prefer(\"count\", \"dplyr\", quiet = TRUE)\nconflict_prefer(\"select\", \"dplyr\", quiet = TRUE)"
  },
  {
    "objectID": "01_data-wrangling.html#fetch-some-sample-data",
    "href": "01_data-wrangling.html#fetch-some-sample-data",
    "title": "1  Data Wrangling Methods",
    "section": "1.2 Fetch Some Sample Data",
    "text": "1.2 Fetch Some Sample Data\nTHIS SECTION WILL EITHER GO OUT ENTIRELY (MOVED TO THE CHAPTER ON IMPORTING CLIMATE DATA), OR BE REPLACED WITH A SIMPLE IMPORT CSV FILE\nFor the examples in this chapter, we’ll work with late-century daily temperature data for a location near Bakersfield, CA in the southern San Joaquin Valley.\n\nbkrfld_cap &lt;- ca_loc_pt(coords = c(-119.151062, 35.261321)) |&gt;\n  ca_gcm(gcms[1:4]) |&gt;                                 \n  ca_scenario(c(\"rcp45\", \"rcp85\")) |&gt;\n  ca_period(\"day\") |&gt;\n  ca_years(start = 2070, end = 2099) |&gt;\n  ca_cvar(c(\"tasmin\", \"tasmax\"))\n\nbkrfld_cap |&gt; ca_preflight()\n\nGeneral issues\n - none found\nIssues for querying values\n - none found\nIssues for downloading rasters\n - none found\n\nplot(bkrfld_cap)\n\n\n\n\n\n\n\n\nFetch data:\n\nbkrfld_tbl &lt;- bkrfld_cap |&gt; \n  ca_getvals_tbl(quiet = TRUE) |&gt; \n  mutate(dt = as.Date(dt),\n         temp_f = units::set_units(val, degF))\n\nhead(bkrfld_tbl)\n\n# A tibble: 6 × 9\n     id cvar   period gcm        scenario spag  dt          val temp_f\n  &lt;int&gt; &lt;fct&gt;  &lt;fct&gt;  &lt;fct&gt;      &lt;fct&gt;    &lt;fct&gt; &lt;date&gt;      [K] [degF]\n1     1 tasmin day    HadGEM2-ES rcp45    none  2070-01-01 278.   41.1\n2     1 tasmin day    HadGEM2-ES rcp45    none  2070-01-02 278.   40.6\n3     1 tasmin day    HadGEM2-ES rcp45    none  2070-01-03 284.   51.7\n4     1 tasmin day    HadGEM2-ES rcp45    none  2070-01-04 276.   37.6\n5     1 tasmin day    HadGEM2-ES rcp45    none  2070-01-05 280.   45.0\n6     1 tasmin day    HadGEM2-ES rcp45    none  2070-01-06 280.   44.5"
  },
  {
    "objectID": "01_data-wrangling.html#working-with-date-and-time-data-chloe",
    "href": "01_data-wrangling.html#working-with-date-and-time-data-chloe",
    "title": "1  Data Wrangling Methods",
    "section": "1.3 Working with Date and Time Data (CHLOE)",
    "text": "1.3 Working with Date and Time Data (CHLOE)\nThis is really important. As part of importing weather data, you often have to do some wrangling with date and time columns. The goals are to\n\nDate columns should be converted to R Date class\nDate-time columns should be saved as POSIXct objects\n\n\n1.3.1 Converting Character Date and Time Columns\nDates and time are often encoded as characters data types. So you have to convert them to R objects. Although this may seem like a pain in the arse, it will save you a lot of pain and suffering.\nlubridate is your friend.\nShow them all the lubridate functions. (see Lubridate cheatsheet)\n\n\n1.3.2 Combining Separate Date and Time Part Columns\nShow code here. Concatenate date part fields with lubridate::make_date() and lubridate::make_datetime()\nCould use CIMIS data as an example (which has columns for ‘month’, ‘year’, and ‘day’)\n\n\n1.3.3 Converting Time Zones\nPretty common to get datetimes values in UTC.\nNote there’s a difference between assigning time zones and converting POSIXct objects to different time zones.\n“Pacific Time”, “PST”, “PDT”, etc. are not recognized time zone names. To see a list of recognized time zone values, you can run:\nOlsonNames() (there’s also a lubridate function that does the same thing)\nlubridate::ymd_hms(x, tz = “America/Los_Angeles”)\n## this will\n## i) convert x (a character object) to POSIXct. If ‘x’ doesn’t have the time zone represented it will assign the local time zone\n## 2) becase we provided tz, it will then convert (add or subtract) the time value to make it Pacific time\n## 3) assign Pacific Time to the result\nThis assigns a time zone, but does not change the value: as.POSIXct(dt, tz=“Africa/Johannesburg”)\n\n\n1.3.4 Saving Date and Time Data Locally\nIf you need to save your weather data to disk, use a format that knows how to save dates and times (e.g., geopackage, native R data file).\nIf you need to save it in a text format (e.g., csv) or as spreadsheet (e.g., xlsx, Google Sheet), use lubridate::format_ISO8601() to write, and lubridate::ymd_hms() to bring it back in."
  },
  {
    "objectID": "01_data-wrangling.html#units",
    "href": "01_data-wrangling.html#units",
    "title": "1  Data Wrangling Methods",
    "section": "1.4 Units",
    "text": "1.4 Units\nShow them the units package\nHow to convert between F, C, and K"
  },
  {
    "objectID": "01_data-wrangling.html#going-from-long-to-wide",
    "href": "01_data-wrangling.html#going-from-long-to-wide",
    "title": "1  Data Wrangling Methods",
    "section": "1.5 Going from Long to Wide",
    "text": "1.5 Going from Long to Wide\nThis is very common. Most tidyverse packages are designed to work with “long” data, but sometimes you have to convert it to wide to feed into another package, compute a metric based on column arithmetic\nUse tidyr\nExample: CIMIS Data is long, convert to wide"
  },
  {
    "objectID": "01_data-wrangling.html#time-lumping",
    "href": "01_data-wrangling.html#time-lumping",
    "title": "1  Data Wrangling Methods",
    "section": "1.6 Time Lumping",
    "text": "1.6 Time Lumping\ngoing from hourly to daily data"
  },
  {
    "objectID": "01_data-wrangling.html#time-filtering-and-slicing",
    "href": "01_data-wrangling.html#time-filtering-and-slicing",
    "title": "1  Data Wrangling Methods",
    "section": "1.7 Time Filtering and Slicing",
    "text": "1.7 Time Filtering and Slicing\nchanging how we slice and dice time\nin both cases, it is useful to construct date parts\nOften an analysis requires you to slice climate data into specific periods that are meaningful for a specific study. For example, water years start on October 1 and run through the following season. Or you might just be interested in the winter months, when tree crops or bugs are particularly sensitive to temperatures. In this section, we’ll look at different techniques for time-slicing climate data.\nThe general approach is to add a column in the table that identifies the time slice. (If you’re working with rasters, the idea is similar but you add an attribute value to the layer). Once you have the time-slice identifiers in your table, you can easily filter or group on that column to compute summaries for each time-slice.\nlubridate is your ally when it comes to extracting date parts. For example to add columns for date parts like year, month, week, and ordinal date, we can use the standard mutate() with date part functions from lubridate:\n\nbkrfld_dtprts_tbl &lt;- bkrfld_tbl |&gt; \n  mutate(year = lubridate::year(dt),\n         month = lubridate::month(dt),\n         week = lubridate::week(dt),\n         yday = lubridate::yday(dt)) |&gt; \n  select(dt, year, month, week, yday, cvar, gcm, scenario, temp_f)\n\nbkrfld_dtprts_tbl |&gt; slice(1:20)\n\n\n\n  \n\n\n\n\n\nTo plot the distribution of winter time daily minimum temperatures (i.e., to identify frost days), we could use the month column to get only dates in December, January, and February:\n\nbkrfld_wintrmth_lows_tbl &lt;- bkrfld_dtprts_tbl |&gt; \n  filter(cvar == \"tasmin\", month %in% c(12, 1, 2))\n  \ntable(bkrfld_wintrmth_lows_tbl$month)\n\n\n   1    2   12 \n7440 6776 7440 \n\n\n\n\nPlot histogram:\n\nggplot(bkrfld_wintrmth_lows_tbl, aes(x=temp_f)) + \n  geom_histogram() +\n  facet_grid(scenario ~ .) +\n  labs(title = \"Distribution of Winter Nightime Lows, 2070-2099\",\n       subtitle = \"Bakersfield, CA\",\n       caption = paste0(\"GCMs: \", paste(gcms[1:4], collapse = \", \"), \". Months: Dec, Jan, and Feb.\"),\n       x = \"night time low (F)\", y = \"count\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nIf we want use the standard definition of the winter season, we could alternately filter winter days by their ordinal date number (aka Julian date). Winter starts on December 21 (day 355 of non-leap years) and ends on March 20 (day 79).\n\nbkrfld_wintrssn_tbl &lt;- bkrfld_dtprts_tbl |&gt; \n  filter(cvar == \"tasmin\", yday &gt;= 355 | yday &lt;= 79)\n\nggplot(bkrfld_wintrssn_tbl, aes(x=temp_f)) + \n  geom_histogram() +\n  facet_grid(scenario ~ .) +\n  labs(title = \"Distribution of Winter Nightime Lows, 2070-2099\",\n       subtitle = \"Bakersfield, CA\",\n       caption = paste0(\"GCMs: \", paste(gcms[1:4], collapse = \", \"), \". December 21 - March 20.\"),\n       x = \"night time low (F)\", y = \"count\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\nTime slices can also be used for grouping. The following expression computes the average nightly low for the summer months for each RCP (all GCMs and years combined):\n\nbkrfld_dtprts_tbl |&gt; \n  filter(month %in% 6:8, cvar == \"tasmin\") |&gt; \n  group_by(month, scenario) |&gt; \n  summarise(avg_temp = mean(temp_f), .groups = \"drop\") |&gt; \n  mutate(month = month.name[month]) |&gt; \n  tidyr::pivot_wider(id_cols = month, names_from = scenario, values_from = avg_temp)\n\n\n\n  \n\n\n\n\n\nSometimes the time period of interest spans two calendar years. A water year for example starts on October 1 and goes thru the end of September the following year. Some agricultural periods (such as winter dormancy) may also start in the fall and continue into the new year.\nSlicing your data by a time period that spans calendar years is done in the same manner - you add a column to the table for period identifier. Below we add a column for water year (which conventionally are designated by the calendar year in which it ends):\n\nbkrfld_wtryr_tbl &lt;- bkrfld_tbl |&gt; \n  mutate(water_yr = year(dt) + if_else(month(dt) &gt;= 10, 1, 0)) |&gt; \n  select(dt, water_yr, cvar, gcm, scenario, temp_f)\n\nbkrfld_wtryr_tbl |&gt; sample_n(10)"
  },
  {
    "objectID": "01_data-wrangling.html#daily-climate-metrics",
    "href": "01_data-wrangling.html#daily-climate-metrics",
    "title": "1  Data Wrangling Methods",
    "section": "1.8 Daily Climate Metrics",
    "text": "1.8 Daily Climate Metrics\nDELETE THIS\nMany climate analyses require metrics computed on a daily time scale. For example, to see how frost exposure might change over time, we may have to look at the lowest daily temperature. This presents a small conundrum, because climate models are not weather forecasts, and best practices tell us that we don’t look at less than 30 years. But it would be silly to average the daily low temperatures by month or year and use that as a proxy for frost exposure.\nThe general approach in these cases is compute the daily metrics as if you were dealing with observed data, but then to aggregate the metrics over bigger periods of time and space. For example, you could classify each individual day in frost / non-frost, and then count the number of predicted frost days over a 30-year interval."
  },
  {
    "objectID": "01_data-wrangling.html#diurnal-temperature-range",
    "href": "01_data-wrangling.html#diurnal-temperature-range",
    "title": "1  Data Wrangling Methods",
    "section": "1.9 Diurnal Temperature Range",
    "text": "1.9 Diurnal Temperature Range\nDELETE THIS (MOVE TO CHAPTER 3)\nDiurnal Temperature Range (DTR) is the difference between daily min and max temperature (Parker et al. 2022). The magnitude of DTR can impact wine grape development and taste. In the example below, we calculate DTR from November thru February.\nThe first step is to separate the minimum and maximum daily temperatures into separate columns:\n\nbkrfld_wide_tbl &lt;- bkrfld_tbl |&gt; \n  filter(month(dt) %in% c(11,12,1,2)) |&gt; \n  tidyr::pivot_wider(id_cols = c(dt, gcm, scenario), names_from = cvar, values_from = temp_f)\n\nhead(bkrfld_wide_tbl)\n\n\n\n  \n\n\n\nNow we can compute DTR:\n\nbkrfld_dtr_tbl &lt;- bkrfld_wide_tbl |&gt; \n  mutate(dtr = tasmax - tasmin)\n\nbkrfld_dtr_tbl |&gt; head()\n\n\n\n  \n\n\n\n\n\nWe can show the results with a box plot:\n\nggplot(bkrfld_dtr_tbl, aes(x=scenario, y = dtr)) + \n  geom_boxplot() + \n  labs(title = \"Diurnal Temperature Range, 2070-2099\",\n       subtitle = \"Bakersfield, CA\", \n       caption = paste0(\"GCMs: \", paste(gcms[1:4], collapse = \", \"), \". Temporal period: Nov-Feb.\"),\n       x = \"Emission scenarios\", y = \"diurnal temperature range\")"
  },
  {
    "objectID": "01_data-wrangling.html#threshhold-based-metrics",
    "href": "01_data-wrangling.html#threshhold-based-metrics",
    "title": "1  Data Wrangling Methods",
    "section": "1.10 Threshhold Based Metrics",
    "text": "1.10 Threshhold Based Metrics\nMany climate analyses involve a threshold event, such as temperature above or below a certain value. These tend to be easy to compute using an expression that returns TRUE or FALSE. Subsequently, you can count the number of threshold events using sum() (when you sum logical values TRUEs become 1 and FALSE becomes 0). You can also look for ‘runs’ of consecutive TRUE values using rle() (see x.x below).\nTODO: SIMPLY THE EXAMPLE BELOW, BY USING SAMPLE WEATHER DATA ABOVE (NOT MODELED WEATHER DATA FROM CAL-ADAPT)\nBelow we compute the number of ‘Hot Days’ per year, where a Hot Day is defined as the maximum temperature over 38 °C (100.4 °F) (Parker et al. 2022). We need to keep gcm and scenario as we’ll be grouping on those columns next.\n\nbkrfld_hotday_tbl &lt;- bkrfld_tbl |&gt; \n  filter(cvar == \"tasmax\") |&gt; \n  mutate(hotday_yn = temp_f &gt;= units::set_units(38, degC),\n         year = year(dt)) |&gt; \n  select(dt, year, cvar, scenario, gcm, temp_f, hotday_yn)\n\nbkrfld_hotday_tbl |&gt; head()\n\n\n\n  \n\n\n\n\n\nNow we can group by year and scenario to compare how the average number of hot days per year looks for each RCP.\n\nbkrfld_numhd_tbl &lt;- bkrfld_hotday_tbl |&gt; \n  group_by(year, scenario, gcm) |&gt; \n  summarise(num_hotday = sum(hotday_yn))\n\n`summarise()` has grouped output by 'year', 'scenario'. You can override using\nthe `.groups` argument.\n\nbkrfld_numhd_tbl |&gt; head()\n\n\n\n  \n\n\nbkrfld_numhd_tbl |&gt; \n  group_by(year, scenario) |&gt; \n  summarize(avg_hd = mean(num_hotday), .groups = \"drop\") |&gt; \n  tidyr::pivot_wider(id_cols = year, names_from = scenario, values_from = avg_hd)\n\n\n\n  \n\n\n\nSometimes you want to know the number of threshold events during a particular time of the year. For example tree crops are particularly susceptible to frost damage right after they’ve bloomed.\nLet’s compute the number of hot days in June, July and August, which can be particularly bad for nut development. Because we have daily data from 4 GCMs, we have to count the number of hot days for each GCM, and then average those together for each emissions scenario.\n\nbkrfld_sumrhd_tbl &lt;- bkrfld_tbl |&gt; \n  filter(cvar == \"tasmax\", month(dt) %in% c(6,7,8)) |&gt; \n  mutate(hd = temp_f &gt;= units::set_units(38, degC),\n         year = year(dt)) |&gt; \n  select(dt, year, cvar, scenario, gcm, temp_f, hd)\n\nbkrfld_sumrhd_tbl |&gt; head()\n\n\n\n  \n\n\n\n\n\n\nbkrfld_numsumrhd_tbl &lt;- bkrfld_sumrhd_tbl |&gt; \n  group_by(year, scenario, gcm) |&gt; \n  summarise(num_sumrhd = sum(hd), .groups = \"drop\") \n\nbkrfld_numsumrhd_tbl |&gt; head()\n\n\n\n  \n\n\n\n\n\n\nbkrfld_avgnumsumrhd_tbl &lt;- bkrfld_numsumrhd_tbl |&gt; \n  group_by(year, scenario) |&gt; \n  summarise(avg_num_sumrhd = mean(num_sumrhd), .groups = \"drop\")\n\nbkrfld_avgnumsumrhd_tbl |&gt; head()\n\n\n\n  \n\n\n\n\n\n\nbkrfld_avgnumsumrhd_tbl |&gt; \n  tidyr::pivot_wider(id_cols = year, names_from = scenario, values_from = avg_num_sumrhd)"
  },
  {
    "objectID": "01_data-wrangling.html#counting-consecutive-events",
    "href": "01_data-wrangling.html#counting-consecutive-events",
    "title": "1  Data Wrangling Methods",
    "section": "1.11 Counting Consecutive Events",
    "text": "1.11 Counting Consecutive Events\n“Heat spells”, “cold spells”, and “extreme precipitation” events are all defined as consecutive days of a threshold event. The number of consecutive days may vary, but the general technique for identifying ‘spells’ is\n\nRun an expression that tests whether the threshhold was passed for each day, returning a series TRUE or FALSE values.\nPass the TRUE / FALSE values into the rle(), which identifies ‘runs’ of TRUE and FALSE values\nCount the number of runs that meet the minimum duration\n\nTo illustrate this, take the following series of 30 temperature values. We’ll compute the number of heat spells where the temperature was 100 or more for three or more days in a row:\n\nx_temp &lt;- c(96,97,101,98,100,102,101,99,94,89,97,102,104,101,103,99,92,94,88,90,98,101,99,103,104,102,98,97,98,99)\n\nStep 1 is to check to see if each value exceeds the threshold):\n\nx_hot &lt;- x_temp &gt;= 100\nx_hot\n\n [1] FALSE FALSE  TRUE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE  TRUE\n[13]  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE\n[25]  TRUE  TRUE FALSE FALSE FALSE FALSE\n\n\nNext, feed the 30 TRUE/FALSE values into rle() (run-length encoding):\n\nrle_lst &lt;- rle(x_hot)\nrle_lst\n\nRun Length Encoding\n  lengths: int [1:11] 2 1 1 3 4 4 6 1 1 3 ...\n  values : logi [1:11] FALSE TRUE FALSE TRUE FALSE TRUE ...\n\n\nrle() returns a list with two elements. Both elements are vectors of the same length. The lengths element contains the number of contiguous identical elements found in a ‘run’ in the original data. The values element contains the corresponding value of the run (in this case TRUE/FALSE values. Using this info, we can see our original data started with two FALSE values, followed by one TRUE value, followed by one FALSE value, followed by three TRUE values, and so on.\nTo count the number of ‘TRUE’ runs (aka spells) equal to or longer than n days, we can apply a simple expression:\n\nsum(rle_lst$values & rle_lst$lengths &gt;= 3)\n\n[1] 3\n\n\n\n\nUsing these techniques, below we compute the number of heat spells where the temperature was 100 °F or more for three or more days in a row:\n\nbkrfld_sumrhd_tbl &lt;- bkrfld_tbl |&gt; \n  filter(cvar == \"tasmax\", month(dt) %in% c(6,7,8)) |&gt; \n  mutate(hd = temp_f &gt;= units::set_units(38, degC),\n         year = year(dt)) |&gt; \n  select(dt, year, cvar, scenario, gcm, temp_f, hd)\n\nbkrfld_sumrhd_tbl |&gt; head()\n\n\n\n  \n\n\n\nWe have to be a little creative to apply rle() in a data frame that has many time series in it (e.g., multiple years, GCMs, and emission scenarios). Each of the series needs to be fed into rle() individually, and the list returned by rle() will have different lengths. But what we can do is set up a list structure to store the results of rle().\nFirst, we create a grouped tibble. A group tibble is still a tibble, but also has groups of rows invisibly defined. As we shall see shortly, other functions know what to do with those groups.\n\nbkrfld_grps_tbl &lt;- bkrfld_sumrhd_tbl |&gt; \n  group_by(year, scenario, gcm) |&gt; \n  arrange(dt)\n\nglimpse(bkrfld_grps_tbl)\n\nRows: 22,080\nColumns: 7\nGroups: year, scenario, gcm [240]\n$ dt       &lt;date&gt; 2070-06-01, 2070-06-01, 2070-06-01, 2070-06-01, 2070-06-01, …\n$ year     &lt;dbl&gt; 2070, 2070, 2070, 2070, 2070, 2070, 2070, 2070, 2070, 2070, 2…\n$ cvar     &lt;fct&gt; tasmax, tasmax, tasmax, tasmax, tasmax, tasmax, tasmax, tasma…\n$ scenario &lt;fct&gt; rcp45, rcp45, rcp45, rcp45, rcp85, rcp85, rcp85, rcp85, rcp45…\n$ gcm      &lt;fct&gt; HadGEM2-ES, CNRM-CM5, CanESM2, MIROC5, HadGEM2-ES, CNRM-CM5, …\n$ temp_f   [degF] 79.84528 [degF], 106.15573 [degF], 90.13398 [degF], 95.24855…\n$ hd       &lt;lgl&gt; FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, …\n\n\n\n\nNext, we have to write a function that we can feed into group_modify(). If you read the documentation for group_modify(), it says the first two arguments of the function should accept i) a group of rows (as a tibble), ii) the properties of the group (e.g., which year, scenario, and gcm) as a 1-row tibble. our function should also return a tibble, that will be stacked for all the groups. In addition, group_modify() will also automatically include columns for the grouping variables.\nIn our case, all we need is the number of heatspells so the function only has to return a 1x1 tibble.\n\nnum_heatspells &lt;- function(data_tbl, key_tbl, num_days = 3) {\n  ## Feed the hd column into rle()\n  rle_lst &lt;- rle(data_tbl$hd)\n  \n  ## Return a tibble with one row\n  tibble(num_spells = sum(rle_lst$values & rle_lst$lengths &gt;= num_days))\n}\n\n\n\nNow we can apply this function to every group:\n\nbkrfld_numspells_tbl &lt;- bkrfld_grps_tbl |&gt; \n  group_modify(.f = num_heatspells, num_days = 3)\n\nbkrfld_numspells_tbl |&gt; head()\n\n\n\n  \n\n\n\nLastly, we can compute the average number of heatspells per emissions scenario:\n\nbkrfld_numspells_tbl |&gt; \n  group_by(scenario) |&gt; \n  summarise(avg_spells = mean(num_spells)) \n\n\n\n  \n\n\n\n\n\n\n\nParker, Lauren E., Ning Zhang, John T. Abatzoglou, Steven M. Ostoja, and Tapan B. Pathak. 2022. “Observed Changes in Agroclimate Metrics Relevant for Specialty Crop Production in California.” Agronomy 12 (1): 205. https://doi.org/10.3390/agronomy12010205."
  },
  {
    "objectID": "02_weather-data.html#summary-for-co-authors",
    "href": "02_weather-data.html#summary-for-co-authors",
    "title": "2  Weather Data",
    "section": "2.1 Summary (for co-authors)",
    "text": "2.1 Summary (for co-authors)\n\nThis chapter provides a short intro to weather station data, with code examples for importing station data.\nThe main goal of the chapter is to provide code recipes for importing, cleaning and saving weather data. The data files (csvs) generated by the examples in this chapter will be saved within the repo, for use in other chapters\nThe general data wrangling techniques / R packages used in this chapter are discussed in Chapter 1.\nWe do not plan to discuss weather variables in any detail. Most of the examples in this book use temperature and precipitation (because those are the most common inputs into agroclimate metrics). This can be expanded in the future. We can provide links to other resources to learn more about weather variables.\nThis chapter focuses on importing station data (i.e., tabular). Interpolted weather raster data will be covered in Chapter 6. Agroclimate rasters. Modeled weather data from climate models (also raster) will be covered in ch 7. modeled climate data.\nFuture topics\n\ncleaning weather data (filling in missing values, range checks)\nsee also https://ucanr-igis.github.io/agroclimR/slides/agclimr_slides01.html#(12)"
  },
  {
    "objectID": "02_weather-data.html#weather-data-intro",
    "href": "02_weather-data.html#weather-data-intro",
    "title": "2  Weather Data",
    "section": "2.2 Intro",
    "text": "2.2 Intro\nComputing agroclimate metrics starts with weather data - a time series of variables such as temperature and precipitation. These could be based on actual measurements from a weather station, or generated from a weather model.\n\n\n\n\n\n\nComputing agroclimate metrics from modeled weather data\n\n\n\nIt may seem surprising that either observed (i.e., ‘real’) or modeled (i.e., computer generated) weather data can be used to compute agroclimate metrics. What does it mean, and why would someone want to, compute something like degree days with modeled (i.e., computer generated) data?\nFor the purposes of computation, the metrics don’t care where the data come from. If you give the degree day equation a record of temperature values, it will give you back the metric. It doesn’t care whether the data are ‘real’ or ‘made up’. But of course what you do with that metric depends a great deal on where the weather data came from.\nMetrics computed from actual measurements, and metrics computed from weather models, are used for different purposes. If you’re a grower, you probably wouldn’t want to schedule your irrigation based on the simulated weather from a climate model. Likewise, if you’re a water control engineer, you probably wouldn’t want to plan the size of your flood infrastructure for the next 50 years based solely on the weather from the past couple of years.\nEach type of weather data has appropriate and inappropriate uses. But the good news is that agroclimate metrics are generally based on plant and insect physiology, so they work for all kinds of data past, present, and future. A metric that predicts nut development for a particular cultivar today is still a pretty good guess for nut development rates in the past as well as 50 years from now.\n\n\nThere are a few characteristics of weather data to be aware of:\nVariables. Weather data typically consists of one or more variables, such as temperature, precipitation, or solar radiation. Some variables are measured directly. Others may be based on models (such as evapotranspiration).\n\n\n\n\n\n\nCIMIS Weather Variables\n\n\n\nThe weather variables recorded by weather stations in the CIMIS network (Section 2.3.1 below) include:\n\ncimir::cimis_items() |&gt; dplyr::pull(Name) |&gt; unique()\n\n [1] \"Average Air Temperature\"   \"Maximum Air Temperature\"  \n [3] \"Minimum Air Temperature\"   \"Dew Point\"                \n [5] \"CIMIS ETo\"                 \"ASCE ETo\"                 \n [7] \"ASCE ETr\"                  \"Precipitation\"            \n [9] \"Average Relative Humidity\" \"Maximum Relative Humidity\"\n[11] \"Minimum Relative Humidity\" \"Average Soil Temperature\" \n[13] \"Maximum Soil Temperature\"  \"Minimum Soil Temperature\" \n[15] \"Average Solar Radiation\"   \"Net Solar Radiation\"      \n[17] \"Average Vapor Pressure\"    \"Maximum Vapor Pressure\"   \n[19] \"Minimum Vapor Pressure\"    \"Wind East-North-East\"     \n[21] \"Wind East-South-East\"      \"Wind North-North-East\"    \n[23] \"Wind North-North-West\"     \"Wind Run\"                 \n[25] \"Average Wind Speed\"        \"Wind South-South-East\"    \n[27] \"Wind South-South-West\"     \"Wind West-North-West\"     \n[29] \"Wind West-South-West\"      \"Air Temperature\"          \n[31] \"Net Radiation\"             \"Relative Humidity\"        \n[33] \"Resultant Wind\"            \"Soil Temperature\"         \n[35] \"Solar Radiation\"           \"Vapor Pressure\"           \n[37] \"Wind Direction\"            \"Wind Speed\"               \n\n\n\n\nTemporal resolution. The time resolution of weather data depends on the source. Measurements from a modern weather station could be recorded as often as every 5 minutes. That doesn’t necessarily mean you’ll have access to data every 5 minutes (which is generally overkill anyway), unless of course you manage the station. Organizations that run weather stations typically provide the data in hourly and/or daily time steps, depending on the needs of their customers. Older data are often only available as daily averages.\n\n\n\n\n\n\nAre more data always better?\n\n\n\n\n\nWhen it comes to weather data for computing agroclimate metrics, more is not necessarily better. Many agroclimate metrics, and more importantly the decision making models derived from those metrics, were developed based on daily data. Hence, even if you have 5-minute data, to use those decision support models that tell you for example when you can turn down the irrigation, you’ll have to resample the 5-minute data to daily averages anyway.\nIn general, try to download the data at the time interval needed for your application.\nSome notable agroclimate metrics that really require hourly data are chill portions and growing degree hours. Changing the time resolution of data is discussed in Section 2.5 below."
  },
  {
    "objectID": "02_weather-data.html#weather-data-sources",
    "href": "02_weather-data.html#weather-data-sources",
    "title": "2  Weather Data",
    "section": "2.3 Weather Data Sources",
    "text": "2.3 Weather Data Sources\n\n\n\n\n\n\nBest Practices for Importing Weather Data\n\n\n\n\nDate columns should be converted to R Date class\n\nDate-time columns should be saved as R POSIXct objects, with the timezone assigned (local time is generally preferable)\n\nUnits should be indicated in column names, accompanying metadata, or by saving values as a units object.\n\n\n\n\n2.3.1 CIMIS Station Data\nCIMIS is a network of about 150 weather stations in California, operated by the CA Dept. of Water Resources. The network was established to inform irrigation management, hence the stations are mostly in agricultural regions.\nCIMIS stations are a bit different from other networks in that:\n\nstations are located on grass fields (a golf course would be a perfect place for a CIMIS station)\nthe stations record evapotranspiration (actually, modeled evapotranspiration) as one of the ‘weather’ variables\n\nThey do this because the ‘reference’ evapotranspiration coming off the grass can be used to estimate evapotranspiration of various crops (by multiplying the reference ET by coefficients determined from research). In turn, the estimated crop ET can be used to compute the water replenishment needs.\nCIMIS data are widely used for many other agricultural applications as well. It is a popular dataset in part because its freely available and can be accessed through the CIMIS website as well as an API.\n\n2.3.1.1 CIMIS API\nTo import CIMIS data into R via the API, you can use the cimir package (Koohafkan 2021). You’ll also need:\n\na CIMIS API key (which you can get by creating a free CIMIS account here)\nthe id number(s) of the weather station(s) of interest (which you can find on the CIMIS website)\nthe abbreviated names of weather variables (see cimis_items())\n\ncimir provides R functions to import data using the CIMIS API. The main download function is cimir::cimis_data(), which can be used to download a range of weather variables (depending on the items argument) at both daily and hourly intervals.\n\n\n2.3.1.2 Import CIMIS Daily Data\nBelow, we’ll download one year of daily data for CIMIS Station 227 (located in Plymouth, Amador County). We begin by loading the packages we’ll be using for data wrangling:\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(lubridate)\nlibrary(stringr)\nlibrary(readr)\nlibrary(cimir)\n\nBelow, we’ll use cimis_data() to get daily precipitation and temperature:\n\n## Step 1. Load my cimis key\nmy_cimis_key &lt;- readLines(\"~/My Keys/cimis_webapi.txt\", n=1)\ncimir::set_key(my_cimis_key)\n\n### Query CIMIS data (Plymouth station #227)\ncim_ply_lng_tbl &lt;- cimir::cimis_data(targets = 227, \n                                     start.date = \"2023-01-01\", \n                                     end.date = \"2023-12-31\",\n                                     measure.unit = \"E\",\n                                     items = \"day-precip,day-air-tmp-max,day-air-tmp-min\")\n\nhead(cim_ply_lng_tbl)\n\n\n\n  \n\n\n\n\n\nAs can be seen above, cimir::cimis_data() returns a tibble in a ‘long’ format. R generally likes long data, but for our purposes of saving it as a CSV file, we’ll next use tidyr::pivot_wider() to reshape it in the more traditional wide format:\n\ncimis227_dly_tbl &lt;- cim_ply_lng_tbl |&gt; \n  select(Station, Date, Item, Value) |&gt; \n  tidyr::pivot_wider(id_cols = c(Station, Date), names_from = Item, \n                     values_from = Value) |&gt; \n  rename(stn_id = Station, date = Date, tmax_f = DayAirTmpMax, tmin_f = DayAirTmpMin, \n         precip_in = DayPrecip)\n\nhead(cimis227_dly_tbl)\n\n\n\n  \n\n\n\n\n\nFinally, we can save the tibble to disk:\n\nwrite_csv(cimis227_dly_tbl, file = \"./data/cimis227_dly.csv\")\n\n\n\n\n\n\n\nCode to import the daily data CSV file\n\n\n\n\n\nTo use these data in examples, you can import the CSV file with either of the following:\nImport from online (works from anywhere):\n\ncimis227_dly_tbl &lt;- readr::read_csv(\"https://github.com/ucanr-igis/xxx\", \n                                   col_types = \"cDddd\")\n\n\n\nIf you are working in the RStudio project for this e-book (e.g., authors), use:\n\ncimis227_dly_tbl &lt;- readr::read_csv(\"./data/cimis227_dly.csv\", \n                                   col_types = \"cDddd\")\n\n\n\n\n\n\nVisualize daily temperature\n\nlibrary(ggplot2)\nggplot(cimis227_dly_tbl, aes(x = date, y = tmin_f)) + \n  geom_line() + \n  xlab(NULL) +\n  ylab(\"temp (F)\") +\n  labs(title = \"Minimum Daily Temperature\",\n       subtitle = \"CIMIS Station #227 - Plymouth\")\n\n\n\n\n\n\n2.3.1.3 CIMIS Hourly Data\nCIMIS stations record measurements hourly. You can import hourly data with cimir, but the maximum number of records you can get from the API in one call is 1750 (about 5 months of data if you’re getting two variables every hour). Thus if you wanted an entire year of hourly data, you would have to make multiple calls for shorter periods and stack them together.\nAn alternative way to download hourly data is to use the CIMIS Station Reports tool on the CIMIS website. This tool allows you to specify station(s), start and end date(s), and weather variables. You can then get the data as a web report, csv, xml, or pdf.\n\n\n\n\n\n\nSign-in before generating CIMIS Station reports\n\n\n\nTo get the most options with CIMIS Station Reports, be sure to be logged-in with your CIMIS account. If you’re not logged in, you can still create station reports, but you’ll be limited to a few preset “limited reports”, which only go back 7 days and are only available to view in your browser.\n\n\nBelow, we’ll import a CIMIS Station Report containing one year of hourly air temperature and precipitation measurements for CIMIS station 227 - Plymouth (Amador County).\n\nstation_report_fn &lt;- \"./data/cimis_plymouth_station-report-2023.csv\"\n\n## Import the CSV file using readr and dplyr\ncim_plymouth_hourly_tbl &lt;- readr::read_csv(station_report_fn,\n                                           col_types = \"ncccccdcdc\") |&gt; \n  rename(stn_id = `Stn Id`, station_name = `Stn Name`, region = `CIMIS Region`,\n         date_chr = Date, hour_pst = `Hour (PST)`, yday = Jul,\n         precip_in = `Precip (in)`, precip_qc = `qc...8`,\n         temp_f = `Air Temp (F)`, temp_qc = `qc...10`) |&gt; \n  mutate(stn_id = as.character(stn_id))  ## for consistency with other datasets\n\nNew names:\n• `qc` -&gt; `qc...8`\n• `qc` -&gt; `qc...10`\n\nhead(cim_plymouth_hourly_tbl)\n\n\n\n  \n\n\n\n\n\nNext, we’ll create a cleaned-up version that we can save. First thing is to combine the Date and Hour columns into a R date-time class (POSIXct):\n\ncimis227_hrly_tbl &lt;- cim_plymouth_hourly_tbl |&gt; \n  mutate(datetime = mdy_h(paste(date_chr, as.numeric(hour_pst) / 100), \n                          tz = \"America/Los_Angeles\")) \n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `datetime = mdy_h(paste(date_chr, as.numeric(hour_pst)/100), tz\n  = \"America/Los_Angeles\")`.\nCaused by warning:\n!  1 failed to parse.\n\nhead(cimis227_hrly_tbl)\n\n\n\n  \n\n\n\n\n\n\n\n\n\nDaylight Savings Time\n\n\n\nWhen you’re switching the time zone of hourly data, you might get a parsing error if the time period includes the night when daylight savings time starts or ends.\nThe problem occurs because local time takes into account daylight savings. This is normally a good thing. However when the clock ‘falls back’, an entire hour of UTC time gets mapped onto the same local time. Similiarly on the night when you jump forward, 2:00am doesn’t exist in local time.\nThe workaround is to keep your data in UTC, or just ignore the warning.\n\n\n\n\nLastly, we can save the data as a csv file:\n\n\n\n\n\n\nSaving date-times in CSV files\n\n\n\nTo save date-time values in CSV files (which are just text), a good practice is to format it in ISO8601 format, which is a well-established open standard for formatting time values as text, including timezone.\nlubridate provides a function format_ISO8601() which makes this easy. The usetz argument appends the time offset from UTC, which import functions can use to figure out the time zone.\n\nlubridate::format_ISO8601(Sys.time(), usetz = TRUE)\n\n[1] \"2024-02-10T16:29:13-0800\"\n\n\nYou can import date-times formatted as ISO8601 back into R using lubridate::ymd_hms().\n\n\n\ncimis227_hrly_tbl |&gt; \n  mutate(dt = format_ISO8601(datetime, usetz = TRUE)) |&gt; \n  select(stn_id, dt, temp_f, precip_in) |&gt; \n  write_csv(\"./data/cimis227_hrly.csv\")\n\n\n\n\n\n\n\nCode to import the hourly data CSV file\n\n\n\n\n\nTo import these data into R, you can run one of the following:\nImport from online (works from anywhere):\n\ncimis227_hrly_tbl &lt;- readr::read_csv(\"https://github.com/ucanr-igis/xxx\", \n                                   col_types = \"Dddd\")\n\nIf you are working in the RStudio project for this e-book (e.g., authors):\n\ncimis227_hrly_tbl &lt;- readr::read_csv(\"./data/cimis227_hrly.csv\", \n                                   col_types = \"Dddd\")\n\n\n\n\n\n\n\n\n\n2.3.2 Synoptic\nShow how to get weather data from a weather station on Synoptic,\nhttps://ucanr-igis.github.io/sketchbook/synoptic-cimis.html\n\n\n\n\n2.3.3 gridMet & Prism\nPRISM and gridMet are rasters of weather variables, interpolated from observed weather data.\ngridMet is available via the Cal-Adapt API (up thru 2021?)\nPRISM is available as NetCDF (I think). Only the 4k product is free.\nPut these in Chapter 6 (will use stars to work with)"
  },
  {
    "objectID": "02_weather-data.html#sec-weather-data-cleaning-data",
    "href": "02_weather-data.html#sec-weather-data-cleaning-data",
    "title": "2  Weather Data",
    "section": "2.4 Cleaning Weather Data",
    "text": "2.4 Cleaning Weather Data\n\n2.4.1 Dealing with bad measurements\nrange checks\n\n\n2.4.2 Missing data\nsee https://inresgb-lehre.iaas.uni-bonn.de/chillR_book/filling-gaps-in-temperature-records.html"
  },
  {
    "objectID": "02_weather-data.html#sec-weather-data-temp-res",
    "href": "02_weather-data.html#sec-weather-data-temp-res",
    "title": "2  Weather Data",
    "section": "2.5 Changing the Temporal Resolution of Weather Data",
    "text": "2.5 Changing the Temporal Resolution of Weather Data\n\n2.5.1 Short to Long\nIf you have timeseries data at short intervals (e.g. hourly), summarizing them by longer intervals (e.g., daily) is technically pretty straightforward using group_by() and summarise() from dplyr. However you have to think carefuly about which summary function is appropriate for the variable you’re interested in. The usual suspects are average, min, max.\n\n\n\n\n\n\nGetting the time zone right\n\n\n\nIf you’re summarizing hourly data (or shorter) to daily intervals, and you’re using the traditional definition of a ‘day’ as starting and ending at midnight, then you’ll want to make sure your data are in local time. Otherwise you’ll be computing 24-hour summaries that start and end at 2pm (if your station is in the western USA, for example).\n\n\nTODO: Give an example of using dplyr to group hourly CIMIS data into daily. Compare with the daily CIMIS data.\n\n\n2.5.2 Interpolating Shorter Intervals\nGoing from daily values to hourly values,is less straightforward and requires a model for the downscaling. Most (all) weather variables don’t vary linearly over the course of the day, nor do they vary consistently over the seasons.\nFor more info, see:\nhttps://inresgb-lehre.iaas.uni-bonn.de/chillR_book/making-hourly-temperatures.html\n\n\n\n\nKoohafkan, Michael. 2021. Cimir: Interface to the CIMIS Web API. https://github.com/mkoohafkan/cimir."
  },
  {
    "objectID": "03_simple-agroclim-metrics.html#load-packages",
    "href": "03_simple-agroclim-metrics.html#load-packages",
    "title": "3  Simple AgroClimate Metrics",
    "section": "3.1 Load Packages",
    "text": "3.1 Load Packages\nAs usual, we start by loading a bunch of packages into memory and specifying our preferences for conflicting function names:\n\nlibrary(caladaptr)\nlibrary(units)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(lubridate)\nlibrary(sf)\n\nSet conflicted preferences:\n\nlibrary(conflicted)\nconflict_prefer(\"filter\", \"dplyr\", quiet = TRUE)\nconflict_prefer(\"count\", \"dplyr\", quiet = TRUE)\nconflict_prefer(\"select\", \"dplyr\", quiet = TRUE)"
  },
  {
    "objectID": "03_simple-agroclim-metrics.html#fetch-some-data",
    "href": "03_simple-agroclim-metrics.html#fetch-some-data",
    "title": "3  Simple AgroClimate Metrics",
    "section": "3.2 Fetch Some Data",
    "text": "3.2 Fetch Some Data\nTo compute agroclimate metrics, we’ll first get 20 years of observed data from the gridMet dataset for a location in Colusa County in the Sacramento Valley.\n\ncolusa_cap &lt;- ca_loc_pt(coords = c(-122.159304, 39.289291)) %&gt;%\n  ca_slug(c(\"tmmn_day_gridmet\", \"tmmx_day_gridmet\")) %&gt;%\n  ca_years(start = 2000, end = 2020) \n\ncolusa_cap\n\nCal-Adapt API Request\nLocation(s): \n  x: -122.159\n  y: 39.289\nSlug(s): tmmn_day_gridmet, tmmx_day_gridmet\nDates: 2000-01-01 to 2020-12-31\n \n\ncolusa_cap %&gt;% ca_preflight()\n\nGeneral issues\n - none found\nIssues for querying values\n - none found\nIssues for downloading rasters\n - none found\n\nplot(colusa_cap)\n\n\n\n\n\n\n\n\nNext we fetch the data. While we’re at it, we’ll add columns for the climate variable (based on the slug), year, and temperature in Fahrenheit:\n\ncolusa_tbl &lt;- colusa_cap %&gt;% \n  ca_getvals_tbl() %&gt;% \n  mutate(dt = as.Date(dt),\n         year = year(as.Date(dt)),\n         cvar = substr(slug, 1, 4),\n         temp_f = units::set_units(val, degF)) %&gt;% \n  select(year, dt, cvar, temp_f)\n\nglimpse(colusa_tbl)\n\nRows: 15,342\nColumns: 4\n$ year   &lt;dbl&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 200…\n$ dt     &lt;date&gt; 2000-01-01, 2000-01-02, 2000-01-03, 2000-01-04, 2000-01-05, 20…\n$ cvar   &lt;chr&gt; \"tmmn\", \"tmmn\", \"tmmn\", \"tmmn\", \"tmmn\", \"tmmn\", \"tmmn\", \"tmmn\",…\n$ temp_f [degF] 32.45 [degF], 30.11 [degF], 32.09 [degF], 39.11 [degF], 32.27 …"
  },
  {
    "objectID": "03_simple-agroclim-metrics.html#growing-degree-days",
    "href": "03_simple-agroclim-metrics.html#growing-degree-days",
    "title": "3  Simple AgroClimate Metrics",
    "section": "3.3 Growing Degree Days",
    "text": "3.3 Growing Degree Days\nGrowing Degree Days (GDD) are a measure of accumulated heat starting from a specific date / event. You may wonder - what’s the point of tracking accumulated heat, given that it cools down every night? The answer is because many plants seem to keep track of accumulated heat units. Research has shown that many phenological events, like the emergence of fruit, are strongly correlated with accumulated heat, also known as thermal time. Insect phenology is likewise strongly correlated with heat units.\nThere are a few ways of computing GDD. Parker et al. (2022) recommend the simple average method with a base temperature of 10 °C:\n\n\\(GDD = (temp_{max} - temp_{min}) / 2 - temp_{base}\\)\n\nNegative GDD values are not biologically meaningful (i.e., plant development generally doesn’t go backwards), so negative GDD values are generally converted to 0 (i.e., Method 1 described by McMaster and Wilhelm (1997)).\nComputing daily GDD is fairly straight-forward. We just have to remember to zero-out negative GDD values:\n\n(tbase_c &lt;- set_units(10, degC))                           ## base temp\n\n10 [°C]\n\ncolusa_gdd_tbl &lt;- colusa_tbl %&gt;% \n  mutate(temp_c = set_units(temp_f, degC)) %&gt;%             ## create a degC column\n  pivot_wider(id_cols = c(year, dt),                       ## make min and max temps separate colums\n              names_from = cvar, \n              values_from = temp_c) %&gt;% \n  mutate(gdd = as.numeric(((tmmx+tmmn)/2) - tbase_c)) %&gt;%  ## compute gdd as a numeric value\n  mutate(gdd = if_else(gdd &lt; 0, 0, gdd))                   ## zero-out negative values\n\ncolusa_gdd_tbl %&gt;% head()\n\n\n\n  \n\n\n\n\n\nApplying GDD to predict crop development requires 1) a start date (also known as a biofix date), and 2) a crop phenology table. These are not shown here, but are not hard to apply (for an example see the Pistachio Nut Development Decision Support tool)."
  },
  {
    "objectID": "03_simple-agroclim-metrics.html#chill-accumulation",
    "href": "03_simple-agroclim-metrics.html#chill-accumulation",
    "title": "3  Simple AgroClimate Metrics",
    "section": "3.4 Chill Accumulation",
    "text": "3.4 Chill Accumulation\nChill accumulation is similar to growing degree days, but for cold. In other words, there are a few phenological events that appear to be strongly correlated with the accumulated amount of chill. An example of this is flowering for many tree crops. Apparently, the trees keep an internal ledger of how cold its been during the winter, and for how long. They use this internal record use to decide when it’s time to come out of their winter dormancy and start flowering. This mechanism probably evolved to help them avoid frost damage.\nResearchers have looked at a number of ways to measure accumulated chill, and the one that does the best job at predicting phenology events is called Chill Portions (CP) (Luedeling and Brown 2011). The calculations are a bit complicated, but fortunately there’s a R-package that will compute chill portions. For more info, see here."
  },
  {
    "objectID": "03_simple-agroclim-metrics.html#frost-days",
    "href": "03_simple-agroclim-metrics.html#frost-days",
    "title": "3  Simple AgroClimate Metrics",
    "section": "3.5 Frost Days",
    "text": "3.5 Frost Days\nFrost Days (FD) are the number of days per year with minimum temperatures (Tn) ≤ 0 °C (Parker et al. 2022). They can be computed with:\n\ncolusa_fd_tbl &lt;- colusa_tbl %&gt;% \n  filter(cvar == \"tmmn\", temp_f &lt;= set_units(0, degC)) %&gt;% \n  group_by(year) %&gt;% \n  summarise(fd = n())\n\ncolusa_fd_tbl"
  },
  {
    "objectID": "03_simple-agroclim-metrics.html#last-spring-and-first-fall-freeze",
    "href": "03_simple-agroclim-metrics.html#last-spring-and-first-fall-freeze",
    "title": "3  Simple AgroClimate Metrics",
    "section": "3.6 Last Spring and First Fall Freeze",
    "text": "3.6 Last Spring and First Fall Freeze\nThe Last Spring Freeze (LSF) is defined as the last day of the calendar year prior to 30 June with a Tn ≤ 0 °C. Conversely the First Fall Freeze (FFF) is defined as the first day of the calendar year commencing 1 July with Tn ≤ 0 °C (Parker et al. 2022).\nWe can find the last freeze date by chaining together dplyr expressions that i) keep only ‘freeze days’ from January through June, ii) group the freeze days by year, and iii) taking the max date for each group:\n\ncolusa_lf_tbl &lt;- colusa_tbl %&gt;% \n  filter(cvar == \"tmmn\", month(dt) &lt;= 6, temp_f &lt;= set_units(32, degF)) %&gt;% \n  group_by(year) %&gt;% \n  summarise(lf = max(dt))\n\ncolusa_lf_tbl\n\n\n\n  \n\n\n\n\n\nSimilarly, we can find the first fall freeze by keeping only dates from July - December where the temperature dipped below freezing, then taking the minimum date for each year:\n\ncolusa_fff_tbl &lt;- colusa_tbl %&gt;% \n  filter(cvar == \"tmmn\", month(dt) &gt;= 7, temp_f &lt;= set_units(32, degF)) %&gt;%\n  group_by(year) %&gt;% \n  summarise(fff = min(dt))\n\ncolusa_fff_tbl"
  },
  {
    "objectID": "03_simple-agroclim-metrics.html#freeze-free-season",
    "href": "03_simple-agroclim-metrics.html#freeze-free-season",
    "title": "3  Simple AgroClimate Metrics",
    "section": "3.7 Freeze-Free Season",
    "text": "3.7 Freeze-Free Season\nThe Freeze-Free Season (FFS) is calculated as the difference between the LSF and FFF (FFF [minus] LSF) (Parker et al. 2022). Since we already calculated LSF and FFF, computing the Freeze-Free Season can be done with a simple table join:\n\ncolusa_lf_fff_tbl &lt;- colusa_lf_tbl %&gt;% left_join(colusa_fff_tbl, by = \"year\")\ncolusa_lf_fff_tbl %&gt;% head()\n\n\n\n  \n\n\ncolusa_lf_fff_ffs_tbl &lt;- colusa_lf_fff_tbl %&gt;% mutate(ffs = fff - lf)\ncolusa_lf_fff_ffs_tbl"
  },
  {
    "objectID": "03_simple-agroclim-metrics.html#tropical-nights-and-hot-days",
    "href": "03_simple-agroclim-metrics.html#tropical-nights-and-hot-days",
    "title": "3  Simple AgroClimate Metrics",
    "section": "3.8 Tropical Nights and Hot Days",
    "text": "3.8 Tropical Nights and Hot Days\nTropical Nights (TRN) are calculated as the number of nights per year with Tn &gt; 20 °C (68 °F) (Parker et al. 2022). This can be computed with:\n\ncolusa_tn_tbl &lt;- colusa_tbl %&gt;% \n  filter(cvar == \"tmmn\", temp_f &gt; set_units(20, degC)) %&gt;% \n  group_by(year) %&gt;% \n  summarise(tn = n())\n\ncolusa_tn_tbl\n\n\n\n  \n\n\n\n\n\nHot Days (HD) are defined as when Tx &gt; 38 °C (Parker et al. 2022). The number of hot days per year can be computed with:\n\ncolusa_hd_tbl &lt;- colusa_tbl %&gt;% \n  filter(cvar == \"tmmx\", temp_f &gt; set_units(38, degC)) %&gt;% \n  group_by(year) %&gt;% \n  summarise(hd = n())\n\ncolusa_hd_tbl"
  },
  {
    "objectID": "03_simple-agroclim-metrics.html#extreme-heat-days",
    "href": "03_simple-agroclim-metrics.html#extreme-heat-days",
    "title": "3  Simple AgroClimate Metrics",
    "section": "3.9 Extreme Heat Days",
    "text": "3.9 Extreme Heat Days\nExtreme Heat Days (EHD) are the number of days per year with Tx &gt;98th percentile of summer (June-August) Tx for the 1981–2010 period (Parker et al. 2022). This is similar to HD, but with a threshold value based on the historic record. We can compute the 98th percentile of daily summertime highs with:\n\ncolusa_ehd_thresh &lt;- ca_loc_pt(coords = c(-122.159304, 39.289291)) %&gt;%\n  ca_slug(\"tmmx_day_gridmet\") %&gt;%\n  ca_years(start = 1981, end = 2010) %&gt;% \n  ca_getvals_tbl(quiet = TRUE) %&gt;% \n  filter(month(as.Date(dt)) %in% c(6,7,8)) %&gt;% \n  pull(val) %&gt;% \n  quantile(0.98) %&gt;% \n  set_units(degF)\n\ncolusa_ehd_thresh\n\n106.2176 [degF]\n\n\n\n\nOnce we have that threshold, we can compute Extreme Heat Days with:\n\ncolusa_ehd_tbl &lt;- colusa_tbl %&gt;% \n  filter(cvar == \"tmmx\", temp_f &gt; colusa_ehd_thresh) %&gt;% \n  group_by(year) %&gt;% \n  summarise(hd = n())\n\ncolusa_ehd_tbl"
  },
  {
    "objectID": "03_simple-agroclim-metrics.html#heatwaves",
    "href": "03_simple-agroclim-metrics.html#heatwaves",
    "title": "3  Simple AgroClimate Metrics",
    "section": "3.10 Heatwaves",
    "text": "3.10 Heatwaves\nHeatwave events (HW) are defined as 3+ consecutive days with Tx &gt; 98th percentile of 1981–2010 summer Tx (as in EHD) (Parker et al. 2022). Using the technique described in [Chapter 4 - Counting Consecutive Events][Counting Consecutive Events], we can compute the number of heatwaves per year.\n\nAdd a column for extreme heat day, then create a grouped tibble (by year):\n\n\ncolusa_grpd_tbl &lt;- colusa_tbl %&gt;% \n  filter(cvar == \"tmmx\") %&gt;% \n  mutate(ehd = temp_f &gt; colusa_ehd_thresh) %&gt;%\n  group_by(year) %&gt;% \n  arrange(dt)\n\nglimpse(colusa_grpd_tbl)\n\nRows: 7,671\nColumns: 5\nGroups: year [21]\n$ year   &lt;dbl&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 200…\n$ dt     &lt;date&gt; 2000-01-01, 2000-01-02, 2000-01-03, 2000-01-04, 2000-01-05, 20…\n$ cvar   &lt;chr&gt; \"tmmx\", \"tmmx\", \"tmmx\", \"tmmx\", \"tmmx\", \"tmmx\", \"tmmx\", \"tmmx\",…\n$ temp_f [degF] 54.95 [degF], 53.87 [degF], 57.65 [degF], 49.37 [degF], 57.47 …\n$ ehd    &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n\n\n\n\n\nCreate a function that we can pass to group_modify(), will return the number of heatwaves per group (year):\n\n\nnum_hw &lt;- function(data_tbl, key_tbl, num_days = 3) {\n  rle_lst &lt;- rle(data_tbl$ehd)\n  tibble(num_hw = sum(rle_lst$values & rle_lst$lengths &gt;= num_days))\n}\n\n \n\nApply the heatwave function to the grouped tibble:\n\n\ncolusa_hw_tbl &lt;- colusa_grpd_tbl %&gt;% \n  group_modify(.f = num_hw, num_days = 3)\n\ncolusa_hw_tbl"
  },
  {
    "objectID": "03_simple-agroclim-metrics.html#diurnal-temperature-range",
    "href": "03_simple-agroclim-metrics.html#diurnal-temperature-range",
    "title": "3  Simple AgroClimate Metrics",
    "section": "3.11 Diurnal Temperature Range",
    "text": "3.11 Diurnal Temperature Range\nDiurnal Temperature Range (DTR) is the difference between daily Tx and Tn (Parker et al. 2022). Below we calculate DTR over 1 March to 1 November.\n\ncolusa_dtr_tbl &lt;- colusa_tbl %&gt;% \n  filter(month(dt) %in% 3:10) %&gt;% \n  pivot_wider(id_cols = c(year, dt), names_from = cvar, values_from = temp_f) %&gt;% \n  mutate(dtr = tmmx - tmmn)\n\nhead(colusa_dtr_tbl)\n\n\n\n  \n\n\n\n\nggplot(colusa_dtr_tbl %&gt;% mutate(year = as.factor(year)), aes(x=year, y = dtr)) + \n  geom_boxplot() + \n  labs(title = \"Diurnal Temperature Range, 2000-2020\",\n       subtitle = \"Colusa, CA\", \n       caption = \"Dataset: gridMet. Temporal period: March - October\",\n       x = \"\", y = \"daily temperature range\")"
  },
  {
    "objectID": "03_simple-agroclim-metrics.html#reference-evapotranspiration",
    "href": "03_simple-agroclim-metrics.html#reference-evapotranspiration",
    "title": "3  Simple AgroClimate Metrics",
    "section": "3.12 Reference Evapotranspiration",
    "text": "3.12 Reference Evapotranspiration\nETo is calculated following the FAO Penman–Monteith method (Allen and United Nations 1998). We calculate summer (June-August) average ETo for each year 1981–2020 for our analysis. ETo units are mm (Parker et al. 2022).\n\nMore coming soon…\n\n\n\n\n\nAllen, R. G., and Food and Agriculture Organization of the United Nations, eds. 1998. Crop Evapotranspiration: Guidelines for Computing Crop Water Requirements. FAO Irrigation and Drainage Paper 56. Rome: Food; Agriculture Organization of the United Nations.\n\n\nLuedeling, Eike, and Patrick H. Brown. 2011. “A Global Analysis of the Comparability of Winter Chill Models for Fruit and Nut Trees.” International Journal of Biometeorology 55 (3): 411–21. https://doi.org/10.1007/s00484-010-0352-y.\n\n\nMcMaster, G, and W Wilhelm. 1997. “Growing Degree-Days: One Equation, Two Interpretations.” Agricultural and Forest Meteorology 87 (4): 291–300. https://doi.org/10.1016/S0168-1923(97)00027-0.\n\n\nParker, Lauren E., Ning Zhang, John T. Abatzoglou, Steven M. Ostoja, and Tapan B. Pathak. 2022. “Observed Changes in Agroclimate Metrics Relevant for Specialty Crop Production in California.” Agronomy 12 (1): 205. https://doi.org/10.3390/agronomy12010205."
  },
  {
    "objectID": "04_degday.html#which-one-to-use",
    "href": "04_degday.html#which-one-to-use",
    "title": "4  Degree Days",
    "section": "4.1 Which one to use?",
    "text": "4.1 Which one to use?\nIf you’re using published phenology tables or a degree day model, use the same method as the author used.\nIf you’re a researcher, note:\n\"Since no advantages were noted in the more complicated double\nand corrected methods, the single triangle and single sine\nmethods are preferred since they are less complicated\nprocedures.\" \n(Roltsch et al 1999)\nsummarized at &lt;https://ipm.ucanr.edu/WEATHER/ddeval.html&gt;\nMore coming soon…"
  },
  {
    "objectID": "05_chill.html",
    "href": "05_chill.html",
    "title": "5  Chill",
    "section": "",
    "text": "More coming soon…"
  },
  {
    "objectID": "06_agroclimate-rasters.html",
    "href": "06_agroclimate-rasters.html",
    "title": "6  Agroclimate Rasters",
    "section": "",
    "text": "This chapter will show how to\n\ntake a raster time series of observed (or modeled) weather data\n\n\nSpatial CIMIS\ngridMet\nPRISM\n\n\nConvert it to stars (see caladaptr vignette)\nUse stars::sapply() to compute agroclimate metrics pixel-by-pixel\n\nkey thing is to put the computation in a function that takes a timeseries vector and spits out the agroclimate metric\n\n\nExample: Compute the number of days &gt; 105 degF for one growing season (Apr - Sep) for a small county\n\nVisualize multi-year agroclimate metric rasters with animation, or a slideshow kind of thing"
  },
  {
    "objectID": "07_modeled-climate-data.html",
    "href": "07_modeled-climate-data.html",
    "title": "7  Modeled Climate Data",
    "section": "",
    "text": "Similarities and differences between observed weather data and modeled climate data\nObserved weather data represent a single series of weather outcomes.\n‘Climate’ is the ‘envelope’ or variability of weather. You need at least 30 years of observed weather data to see the envelope.\nModeled climate data are based on simulations of weather (hourly). Just like observed weather data, you need 30 years of simulated weather to characterize the envelope.\nThe simulated weather data from a climate model are not meant to be predictive. But the envelope of many ‘runs’ of the simulation do a good job at predicting the climate envelope.\nYou can compute future agroclimate metrics with projected climate data, provided you have access to the simulated weather (e.g., daily temperature). This is basically the same as computing the metric with observed data.\nThe tricky part comes when analzing the results, because\n\nThere is not one projected climate future, but many (different CGMs, different emissions scenarios, different runs)\nTherefore you have a range of projected agroclimate metrics. You have to characterize them as a group / probabilistically"
  },
  {
    "objectID": "08_climate-analogues.html#climate-analogues",
    "href": "08_climate-analogues.html#climate-analogues",
    "title": "8  AgroClimate Analogues",
    "section": "8.1 Climate Analogues",
    "text": "8.1 Climate Analogues\nThe following is just placeholder text, which needs to be updated with some actual definitions and citations.\nClimate analogues refer to locations that have the same climate profile. For example California and Spain might be considered climate analogues, because they both have temperate, moderate climates, with a similar distribution of rainfall.\nA modeled climate analogue is a location that currently has the climate that is projected to occur somewhere else. For example, Sacramento’s climate analog 50 year from now might be Bakersfield. That means Bakersfield already has the weather envelope that Sacromentons can expect to see 50 years from now.\nClimate analogues are useful because they make the effects of climate change more understandable. For example if I live in Sacramento and want to know what kind of street trees will be viable in 50 years, or what kind of cooling systems will be needed in houses, I don’t have to run fancy models. All I have to do is take a trip to Bakerfield and ‘see the future’.\nBut what does it mean to have “similar” climates? This is where metrics come in, because the answer depends on your use case. If you’re interested in what kind of trees to plant in your orchard, you’d probably want to define ‘similarity’ based on a metric like winter chill. If you’re building an irrigation system, you might want to look at models of evapotranspiration."
  },
  {
    "objectID": "08_climate-analogues.html#example",
    "href": "08_climate-analogues.html#example",
    "title": "8  AgroClimate Analogues",
    "section": "8.2 Example",
    "text": "8.2 Example\nsee McBride and Laćan (2018) (they used the temperature of hottest day in July as a metric for urban tree species viability)\nuse CalAdaptR to compare historical modeled July temps with future historic modeled July temps\ndo this for all preset areas of interest the CalAdapt server - Or I could use a place names layer\n\n\n\n\nMcBride, Joe R., and Igor Laćan. 2018. “The Impact of Climate-Change Induced Temperature Increases on the Suitability of Street Tree Species in California (USA) Cities.” Urban Forestry & Urban Greening 34 (August): 348–56. https://doi.org/10.1016/j.ufug.2018.07.020."
  },
  {
    "objectID": "99_other.html#frost-exposure",
    "href": "99_other.html#frost-exposure",
    "title": "9  Other Content, Future Work",
    "section": "9.1 Frost Exposure",
    "text": "9.1 Frost Exposure\n(Parker, Pathak, and Ostoja 2021) computed frost exposure based on the number of hours within temperature ranges defined by each crop (e.g., 28-30, 30-32, 32-34). They used this metric to compare changes over time.\n\nThe frequency of frost temperatures under observed contemporary and projected future climate conditions were assessed in units of hours of exposure, quantified using daily maximum and minimum temperatures temporally disaggregated to hourly data using a modified sine curve approach (Linvill, 1990)…In assessing frost exposure that is not crop-specific, we highlight the frost exposure (in hours) for three threshold temperatures (T) – -2 °C, 0 °C, and 2 °C – that encapsulate the most frost-tender phase of development for many high-value fruit and nut trees (e.g., Gholipour, 2006; WSU, 2020)."
  },
  {
    "objectID": "99_other.html#pest-development---chained-analyses",
    "href": "99_other.html#pest-development---chained-analyses",
    "title": "9  Other Content, Future Work",
    "section": "9.2 Pest Development - Chained Analyses",
    "text": "9.2 Pest Development - Chained Analyses\nThese authors looked at generation times, number of generations per season, biofix events, and phenology stages. These analyses are based on chaining degree day models.\n(Pathak, Maskey, and Rijal 2021)\nThey used degree day models to compare past and future patterns of NOW development (timing, generation length, number of generations).\n\nThe objective of this study was to quantify changes in the biofix, lifecycle length, and number of generations for these pests for the entire Central Valley of California. Using a well-established growing-degree days (GDD) model calibrated and validated using observations from orchards of California, and climate change projections from the Coupled Model Intercomparison Project phases 5 and 6 (CMIP5 and CMIP6) General Circulation Models, we found that biofix dates of these pests are expected to shift earlier by up to 28 days, and length of generations is expected to be shortened by up to 19 days, and up to 1.4 extra generations of these pests can be added by the end of the century depending on the scenario.\n\n\n\n(Jha et al. 2024)\nSimilar to above, but more pests.\n\nThe objective of this study was to quantify changes in the biofix, lifecycle length, and number of generations for these pests for the entire Central Valley of California. Using a well-established growing-degree days (GDD) model calibrated and validated using observations from orchards of California, and climate change projections from the Coupled Model Intercomparison Project phases 5 and 6 (CMIP5 and CMIP6) General Circulation Models, we found that biofix dates of these pests are expected to shift earlier by up to 28 days, and length of generations is expected to be shortened by up to 19 days, and up to 1.4 extra generations of these pests can be added by the end of the century depending on the scenario."
  },
  {
    "objectID": "99_other.html#making-info-actionable",
    "href": "99_other.html#making-info-actionable",
    "title": "9  Other Content, Future Work",
    "section": "9.3 Making Info Actionable",
    "text": "9.3 Making Info Actionable\nTURNING VALUES INTO PROBABILITY STATEMENTS\nProbability statements are more actionable for growers, planners, policy makers.\nProbability statement are intrinsic to making sense of our multiple climate futures\nWhat is the likelihood of…\n\naverage temp greater than X between two dates\nnot getting a frost (that might kill vineyard pests) within two Julian dates, hence you might need more pest management\nextreme heat events (defined by maximum daily temp exceeding X degrees Y days in a row)\n\nSee also the chill portions calculator - computes the probability of getting sufficient chill at least 9 years out of 10 (which is what you need for economic viability of an orchard operation)\n\n\n\n\nJha, Prakash Kumar, Ning Zhang, Jhalendra P. Rijal, Lauren E. Parker, Steven Ostoja, and Tapan B. Pathak. 2024. “Climate Change Impacts on Insect Pests for High Value Specialty Crops in California.” Science of The Total Environment 906 (January): 167605. https://doi.org/10.1016/j.scitotenv.2023.167605.\n\n\nParker, Lauren E., Tapan Pathak, and Steven Ostoja. 2021. “Climate Change Reduces Frost Exposure for High-Value California Orchard Crops.” Science of The Total Environment 762 (March): 143971. https://doi.org/10.1016/j.scitotenv.2020.143971.\n\n\nPathak, Tapan, Mahesh Maskey, and Jhalendra P. Rijal. 2021. “Impact of Climate Change on Navel Orangeworm, a Major Pest of Tree Nuts in California.” Science of The Total Environment 755 (February): 142657. https://doi.org/10.1016/j.scitotenv.2020.142657."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Allen, R. G., and Food and Agriculture Organization of the United\nNations, eds. 1998. Crop Evapotranspiration: Guidelines for\nComputing Crop Water Requirements. FAO Irrigation and\nDrainage Paper 56. Rome: Food; Agriculture Organization of the United\nNations.\n\n\nJha, Prakash Kumar, Ning Zhang, Jhalendra P. Rijal, Lauren E. Parker,\nSteven Ostoja, and Tapan B. Pathak. 2024. “Climate Change Impacts\non Insect Pests for High Value Specialty Crops in\nCalifornia.” Science of The Total\nEnvironment 906 (January): 167605. https://doi.org/10.1016/j.scitotenv.2023.167605.\n\n\nKoohafkan, Michael. 2021. Cimir: Interface to the CIMIS Web\nAPI. https://github.com/mkoohafkan/cimir.\n\n\nLuedeling, Eike, and Patrick H. Brown. 2011. “A Global Analysis of\nthe Comparability of Winter Chill Models for Fruit and Nut\nTrees.” International Journal of Biometeorology 55 (3):\n411–21. https://doi.org/10.1007/s00484-010-0352-y.\n\n\nMcBride, Joe R., and Igor Laćan. 2018. “The Impact of\nClimate-Change Induced Temperature Increases on the Suitability of\nStreet Tree Species in California (USA)\nCities.” Urban Forestry & Urban Greening 34\n(August): 348–56. https://doi.org/10.1016/j.ufug.2018.07.020.\n\n\nMcMaster, G, and W Wilhelm. 1997. “Growing Degree-Days: One\nEquation, Two Interpretations.” Agricultural and Forest\nMeteorology 87 (4): 291–300. https://doi.org/10.1016/S0168-1923(97)00027-0.\n\n\nParker, Lauren E., Tapan Pathak, and Steven Ostoja. 2021. “Climate\nChange Reduces Frost Exposure for High-Value California\nOrchard Crops.” Science of The Total Environment 762\n(March): 143971. https://doi.org/10.1016/j.scitotenv.2020.143971.\n\n\nParker, Lauren E., Ning Zhang, John T. Abatzoglou, Steven M. Ostoja, and\nTapan B. Pathak. 2022. “Observed Changes in\nAgroclimate Metrics Relevant for\nSpecialty Crop Production in\nCalifornia.” Agronomy 12 (1): 205. https://doi.org/10.3390/agronomy12010205.\n\n\nPathak, Tapan, Mahesh Maskey, and Jhalendra P. Rijal. 2021.\n“Impact of Climate Change on Navel Orangeworm, a Major Pest of\nTree Nuts in California.” Science of The Total\nEnvironment 755 (February): 142657. https://doi.org/10.1016/j.scitotenv.2020.142657."
  }
]